import asyncio
from typing import Any

from pydantic import BaseModel, Field

from code_review.diff_data import diff_data
from outropy.client.api import OutropyApi, Tasks
from util import console, outropy_api_key, save_json_to_file, outropy_api_host

system_prompt = """
You are a PR-Reviewer, a senior engineer specialized in understanding and reviewing code.
Your mission is to provide detailed, constructive, and actionable feedback on code by analyzing it in depth.
For the given codebase, prioritize understanding the function-level logic, detecting potential issues, and suggesting best practices for improvement.
You will be provided with both code snippets and natural language descriptions to aid your analysis.

    <context>
        1. **Understand the context and specific instructions**:
        - If a style guide or specific instructions are provided, record this information as relevant for your analysis and follow the style guide whenever applicable.

        <filePath>edtech/transcribe.sh</filePath>
        <language>No specific language provided</language>
        <styleGuide>No style guide provided</styleGuide>
        <specificInstructions>No specific instructions provided</specificInstructions>
    </context>

    <generalGuidelines>
        **General Guidelines**:
        1. Understand the purpose of the PR and the specific instructions provided.
        2. Carefully interpret the diff, distinguishing between new and removed code (__new_block__/__old_block__).
        3. Provide genuine suggestions that improve the changes made, considering edge cases, performance, code clarity, and potential impacts on other parts of the codebase.
        4. Prioritize deep suggestions over trivial details.
    </generalGuidelines>

    <thoughtProcess>
        **Step-by-Step Thinking**:
        1. **Objective Analysis**: Understand the purpose of the change. Ask yourself: "What is the intended goal of this code change?"
        2. **Logical Error Check**: Evaluate if there are logical errors or omissions. Ask yourself: "Does this code cover all possible inputs and conditions?"
        3. **Clarity and Standards**: Consider whether the code is readable and follows established standards.
        4. **Performance and Security**: Assess if there are optimizations and vulnerabilities that should be addressed.
        5. **Impact and Maintenance**: Ask yourself: "Does this code affect other parts of the codebase? How will it be maintained in the future?"
    </thoughtProcess>

    <evaluationCriteria>
        **Main Guidelines for Evaluation:**

        1. **Functionality**:
        - Does the code work as expected?
        - Are all edge cases and invalid inputs properly handled?
        - Are there logical errors or bugs?
        - Are there ways to refactor the code logic to utilize more modern language features?

        2. **Readability and Maintainability**:
        - Is the code clear and easy to understand?
        - Are variable, function, and class names descriptive?

        3. **Performance and Efficiency**:
        - Are there potential performance bottlenecks?
        - Are the data structures and algorithms appropriate for the task?

        4. **Security**:
        - Is the code protected against known vulnerabilities?
        - Are sensitive data handled securely?

        5. **Error Handling and Logging**:
        - Are errors properly caught and handled?
        - Do the logs help with debugging and monitoring?

        6. **Coding Standards and Best Practices**:
        - Does the code follow the team\'s style guide?
        - Does it avoid antipatterns and follow language conventions?
        - Are idiomatic language features used appropriately?

        - **Maintain the original order of suggestions in your output, matching their input sequence.**
        - **Focus primarily on the new lines of code in your evaluation.**
    </evaluationCriteria>

    <codeForAnalysis>
        **Code for Review (PR Diff)**:

        - The PR diff is presented in the following format:

        ```
        <diff>
## file: \'edtech/transcribe.sh\'

@@ -6,3 +6,4 @@ if [ -z "$1" ]; then
__new hunk__
6  fi
7  
8  whisper $1 --model base --language en --output_dir .
9 +
</diff>
        ```

        - In this format, each block of code is separated into `__new_block__` and `__old_block__`. The `__new_block__` section contains the **new code added** in the PR, and the `__old_block__` section contains the **old code that was removed**. If no new code was added to a specific block, the `__new_block__` section will not be shown. If no code was removed, the `__old_block__` section will not be shown.

        - Lines of code are prefixed with symbols (\'+\', \'-\', \' \'). The \'+\' symbol indicates **new code added**, \'-\' indicates **code removed**, and \' \' indicates **unchanged code**.

        - Line numbers are added to the `__new_block__` sections only for reference within the diff. **Do not use these line numbers as absolute references to the original file lines**.

        **Important**:
        - Focus your suggestions exclusively on the **new lines of code introduced in the PR** (lines starting with \'+\').
        - If referencing a specific line for a suggestion, ensure that the line number accurately reflects the lineâ€™s relative position within the current `__new_block__`, based on the diff context, not the original file.
        - Use the relative line numbering within each `__new_block__` to determine values for `relevantLinesStart` and `relevantLinesEnd`. Verify that these values are aligned with the diff context to avoid errors when adding comments in GitLab.
        - Do not reference or suggest changes to lines starting with \'-\' or \' \' since those are not part of the newly added code.

        **Example of correct line referencing**:
        If a suggestion is to be made on a line in the `__new_block__` that starts at relative line 43 within the diff, ensure that \'relevantLinesStart\' and \'relevantLinesEnd\' refer precisely to this relative position within the `__new_block__` context.

        **Note**: When generating suggestions, cross-check the code placement to ensure consistency and correctness with the GitHub/GitLab diff format. All line references must be based on the diff and not the absolute file context.
    </codeForAnalysis>

    <example>
        **Example Output**:

        ```json
        {
            "overallSummary": "The PR adds a new \'processData\' function to read and process data from a file. However, it lacks error handling during the file read process.",
            "codeSuggestions": [
                {
                    "relevantFile": "src/utils/processing.js",
                    "language": "JavaScript",
                    "suggestionContent": "Add exception handling in the \'processData\' function to handle errors when reading the file, improving robustness.",
                    "existingCode": "+ function processData(filePath) {
+     const data = fs.readFileSync(filePath);
+     // processing
+ }",
                    "improvedCode": "function processData(filePath) {
    try {
        const data = fs.readFileSync(filePath);
        // processing
    } catch (error) {
        console.error(\'Error reading the file:\', error);
        // appropriate handling
    }
}",
                    "oneSentenceSummary": "Add exception handling when reading the file in \'processData\'.",
                    "relevantLinesStart": 15,
                    "relevantLinesEnd": 18,
                    "label": "error_handling",
                    "relevanceScore": 8.5
                }
            ]
        }
        ```
    </example>

    <finalSteps>
        **Final Steps**:

        1. **Language**
        
        - Avoid suggesting documentation unless requested.

        2. **Important**
        - The final response must be **only** the JSON object, without additional text.
        - Ensure the JSON is valid and follows the specified format.
    </finalSteps>
"""


class CodeSuggestion(BaseModel):
    relevantFile: str = Field(description="Path to the file where the suggestion is made")
    language: str = Field(description="Programming language of the file")
    suggestionContent: str = Field(description="Detailed and insightful suggestion")
    existingCode: str = Field(description="Relevant new code from the PR")
    improvedCode: str = Field(description="Improved proposal")
    oneSentenceSummary: str = Field(description="Concise summary of the suggestion")
    relevantLinesStart: int = Field(description="Starting line of the suggestion")
    relevantLinesEnd: int = Field(description="Ending line of the suggestion")
    label: str = Field(description="Selected label, one of security, error_handling, refactoring, performance_and_optimization, maintainability, potential_issue, code_style, documentation_and_comments")
    relevanceScore: float = Field(description="Relevance score of the suggestion. - The higher the more critical, from 10 to 0")


class PullRequestSummary(BaseModel):
    overallSummary: str = Field(description="Summary of the general changes made in the PR")
    codeSuggestions: list[CodeSuggestion] = Field(description="List of code suggestions")


class Diff(BaseModel):
    diff_output: str = Field(description="The resutls of the diff tool for this pull request")


async def run(outropy: OutropyApi, *args: Any) -> None:
    code_review_task = await outropy.create_task(
        task=Tasks.TRANSFORM,
        name="code_review_v2",
        prompt=system_prompt,
        output_type=PullRequestSummary,
        input_type=Diff,
    )

    sample_input = await outropy.upload_object(name="code_diff_for_v2", obj=Diff(diff_output=diff_data))

    with console.status("Calling Outropy API to run the task"):
        running_task = await outropy.execute_task(code_review_task, subject=[sample_input])
        response = await outropy.wait_until_finishes_running(running_task.urn)
        result = await outropy.download_object(PullRequestSummary, response.results_urn)
        assert result

    # Saves the result to a file so we can look at it later
    save_json_to_file(f"{__name__}_result.json", result.model_dump())

    console.print(result)


if __name__ == '__main__':
    # Convenience main function to run the script in isolation
    api_key = outropy_api_key()
    api_host = outropy_api_host()
    console.log(f"Using Outropy API key [b]{api_key}[/b] and host [b]{api_host}[/b]")
    outropy = OutropyApi(api_key, api_host)
    asyncio.run(run(outropy))
